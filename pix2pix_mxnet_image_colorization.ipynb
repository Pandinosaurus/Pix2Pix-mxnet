{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2pix image colorization. \n",
    "#### Examples using pre-trained generator model and a collection of images that are in the training and outside the training set. Also getting results analyzed via histogram comparsions and attempting to colorize webcam mjpeg stream in real time.\n",
    "\n",
    "https://github.com/skirdey/mxnet-pix2pix - code to train pix2pix using MXNet and Python 3.6, it also visualizes training process via cv2 library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was trained on a set of 36000 256x256 images mostly covering city streets and city scape. The images were extracted from a video at about 4fps. The training process took approximately 20 hours on a laptop with Nvidia Geforce GTX 1060 with 6GB of video memory. \n",
    "\n",
    "Skinny of the tutorial:\n",
    "1. Load an image\n",
    "2. Convert to lab and extract lightness\n",
    "3. Run lightness through pre-trained pix2pix generator\n",
    "4. Display and analyize colorization result\n",
    "5. Colorize video captchured from a live web-cam in real-time and display an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of real-time colorization ( not real time anymore ). Video stream came from a web camera attached to the computer. \n",
    "https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization_street.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization_street.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization_street.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of real-time colorization ( not real time anymore ). Video stream came from an online web camera.\n",
    "https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv2_utils import show_mxnet_to_numpy_array, show_numpy_array\n",
    "from util import rgb_to_lab\n",
    "from util.process_lab_utils_mx import preprocess_lab\n",
    "from util.process_lab_utils_np import lab_parts_to_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary utilities and system modules. We are doing lab colorization, meaning we will convert an image into the lab space and use the lightness channel to run it through a generator. The network, traning, and helper classed can be found here - https://github.com/skirdey/mxnet-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu(0) # Nvidia Geforce GTX 1060 6GB. You should see games run on this beauty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "What we are doing here is loading a pre-trained generator model, and binding data parameters to it.\n",
    "We also have several functions to load an image, either jpeg or png, and get is lightness channel that we can run through generator network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeme(method):\n",
    "    def wrapper(*args, **kw):\n",
    "        startTime = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        endTime = time.time()\n",
    "        print(str(method.__name__) + \": \", endTime - startTime , 'seconds')\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_generator_from_checkpoint():\n",
    "    mod_generator = mx.module.Module.load(\"G\", 7, load_optimizer_states=True, data_names=('gData',),\n",
    "                                               label_names=None, context=ctx)\n",
    "    mod_generator.bind(data_shapes=[('gData', (1,) + (1, 256, 256))], for_training=False)\n",
    "    return mod_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_filename):\n",
    "    assert image_filename\n",
    "    with open(image_filename, 'rb') as fp:\n",
    "        str_image = fp.read()\n",
    "\n",
    "    return mx.img.imdecode(str_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "Get color channels function uses internal helper functions to process an RGB image into LAB space. \n",
    "It also resizes an image into 256 by 256 square so we can run it through the generator network. \n",
    "Before returning the result, we resize the generator's output back to image's original size. \n",
    "As generator expects a square image, it is best to operate on square sample images to test out colorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timeme\n",
    "def generator_run(model, grayscale_lightness):\n",
    "    model.forward(mx.io.DataBatch([grayscale_lightness]), is_train=True)\n",
    "    return model.get_outputs()[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_color_channels(image, model):\n",
    "\n",
    "    w, h = image.shape[0:2]\n",
    "\n",
    "    if image.shape[0:2] != (256, 256):\n",
    "        image = mx.image.resize_short(image, 256)\n",
    "        image = mx.image.center_crop(image, (256, 256))[0]\n",
    "\n",
    "\n",
    "    lightness_chan = get_lightness(image)\n",
    "\n",
    "    grayscale_lightness = nd.expand_dims(lightness_chan, axis=2).transpose((2, 3, 0, 1))\n",
    "\n",
    "    output = generator_run(model, grayscale_lightness)\n",
    "    \n",
    "    a_b_channels = squeeze_mx_image(output).transpose((2,1,0))\n",
    "\n",
    "    a_b_channels = cv2.resize(a_b_channels.asnumpy(), (w, h))\n",
    "    a_b_channels = nd.expand_dims(nd.array(a_b_channels), axis=2).transpose((2, 3, 1, 0))\n",
    "\n",
    "    return a_b_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squeeze_mx_image(image):\n",
    "    return nd.array(np.squeeze(image.asnumpy(), axis=0), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lightness(image):\n",
    "    image = image.as_in_context(ctx)\n",
    "    lab = rgb_to_lab(image, ctx=ctx)\n",
    "    lightness_chan, _, _ = preprocess_lab(lab)\n",
    "    return lightness_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def colorize_image(real_a, model):\n",
    "    color_channels = get_color_channels(real_a, model)\n",
    "    original_size_lightness = get_lightness(real_a)\n",
    "\n",
    "    grayscale_lightness = nd.expand_dims(original_size_lightness, axis=2).transpose((3,2,0,1))\n",
    "\n",
    "    fake_rgb = lab_parts_to_rgb(color_channels.asnumpy() * 3, grayscale_lightness.asnumpy())\n",
    "    \n",
    "    return fake_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moment of truth\n",
    "Let's load an image that I've taken on my phone, cropped it to a square and resized to 512x512. The neural net never seen this picture before. It was quite new for me as well - sunny in Seattle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit on Pix2pix\n",
    "Pix2pix approach is based on two neural networks, that are trained at the same time. \n",
    "\n",
    "Generator network in a original GAN is a model that learns mapping from random noise vector $z$ to output image $y$:\n",
    "\n",
    "$$G : z \\rightarrow y$$\n",
    "\n",
    "Pix2pix uses approach of conditional GANS, where generator learns mapping from original image $x$ and random noise vector $z$ to original image $y$:\n",
    "\n",
    "$$ G : {x, z} \\rightarrow y$$\n",
    "\n",
    "The objective of a pix2pix training is $G^{âˆ—}= arg\\,\\underset{G}{min}\\,\\underset{D}{max}\\,\\mathcal{L}_{cGAN} (G, D) \\,+\\, \\alpha\\times \\mathcal{L}_{L1}(G)$\n",
    "\n",
    "The objective stands for use gradient 'ascent' on Discriminator and *increase* the probability of detecting fakes and recognizes real pictures, and use gradient descent on the generator to decrease the probability to be detected as a fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_generator_from_checkpoint()\n",
    "\n",
    "real = load_image(\"./test_img/IMG-2269.jpg\")\n",
    "colorized =  colorize_image(nd.cast(real, \"uint8\"), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original, straight from the phone\n",
    "real = real.asnumpy() # It has type MXNet.NDArray at this point\n",
    "Image.fromarray(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorized version\n",
    "Image.fromarray(colorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "It is nice to see the result visually. And it is nice to see some quanitifed metrics. We can have rgb histogram comparsions between original and colorized version, and also an L1 difference between the fake and the real images. It also make sense to time the colorization call itself, and see how fast it performs its task. Lets start with it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color = ('r','g','b')\n",
    "plt.figure(figsize=(15,4))\n",
    "for channel, col in enumerate(color):\n",
    "    histr_real = cv2.calcHist([real], [channel], None, [256], [0,256])\n",
    "    histr_colorized = cv2.calcHist([colorized], [channel], None, [256], [0,256])\n",
    "    \n",
    "    plt.xlim([0,256])\n",
    "    plt.ylim([-500, 500])\n",
    "    plt.subplot(1, 3, 1) # plt.subplot allows us to plot side by side, the args are (row, col, current plot)\n",
    "    plt.title(\"Real\")\n",
    "    plt.plot(histr_real,color = col)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Colorized\")\n",
    "    plt.plot(histr_colorized, color = col)\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Difference\")\n",
    "    plt.plot(histr_real - histr_colorized, color = col)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the test, lets make sure the histogram of real image has correlation of 1 \n",
    "to itself, and do the same for the colorized version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the difference numerically using OpenCV compareHist function. \n",
    "https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=comparehist#comparehist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing all three color channels at once\n",
    "histr_real = cv2.calcHist([real], [0,1,2], None, [256, 256, 256], [0,256, 0, 256, 0, 256])\n",
    "histr_colorized = cv2.calcHist([colorized], [0,1,2], None, [256, 256, 256], [0,256, 0, 256, 0, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "cv2.compareHist(histr_real, histr_real, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.compareHist(histr_colorized, histr_colorized, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least we know that correlation functions works on two identical histograms. \n",
    "Lets try it on real and colorized images. Correlatiom method performs following computation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation metric\n",
    "[-1:1], 1 being perfect match\n",
    "\n",
    "$$d(H_1,H_2) =  \\frac{\\sum_I (H_1(I) - \\bar{H_1}) (H_2(I) - \\bar{H_2})}{\\sqrt{\\sum_I(H_1(I) - \\bar{H_1})^2 \\sum_I(H_2(I) - \\bar{H_2})^2}}$$ where $$\\bar{H_k} =  \\frac{1}{N} \\sum _J H_k(J)$$ and $N$ is a total number of bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.compareHist(histr_real, histr_colorized, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-0.27 - Not great, but not bad. The colorized image looks like a badly taken photo, but the colors there do make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square\n",
    "[0: positive infinity], 0 is perfect match\n",
    "It computes $$d(H_1,H_2) =  \\sum _I  \\frac{\\left(H_1(I)-H_2(I)\\right)^2}{H_1(I)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.compareHist(histr_real, histr_colorized, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bhattacharyya distance\n",
    "[-1: 1], 0 is perect match\n",
    "$$d(H_1,H_2) =  \\sqrt{1 - \\frac{1}{\\sqrt{\\bar{H_1} \\bar{H_2} N^2}} \\sum_I \\sqrt{H_1(I) \\cdot H_2(I)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.compareHist(histr_real, histr_colorized, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8798752374385485 - far away from a perfect match. Still, visually colorization result doesn't feel that wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video colorization on the client in a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def signal_handler(signal, frame):\n",
    "    # KeyboardInterrupt detected, exiting\n",
    "    global is_interrupted\n",
    "    is_interrupted = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_more_url = \"http://217.197.157.7:7070/axis-cgi/mjpg/video.cgi?resolution=320x240\"\n",
    "url = \"http://85.46.64.147/axis-cgi/mjpg/video.cgi?camera=&resolution=320x240\" # looks like italy\n",
    "vc = cv2.VideoCapture(url)\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    is_capturing, frame = vc.read()\n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "    frame_colorized = colorize_image(nd.array(frame), model)\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    webcam_preview_original = plt.imshow(frame)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    webcam_preview_colorized = plt.imshow(frame_colorized)    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "is_interrupted = False\n",
    "while is_capturing:\n",
    "    is_capturing, frame = vc.read() \n",
    "    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "    frame_colorized = colorize_image(nd.array(frame), model)\n",
    "    \n",
    "    webcam_preview_original.set_data(frame)\n",
    "    webcam_preview_colorized.set_data(frame_colorized)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    try:    # Avoids a NotImplementedError caused by `plt.pause`\n",
    "        plt.pause(0.05)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if is_interrupted:\n",
    "        vc.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look at a recorded gif once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://s3-us-west-1.amazonaws.com/pix2pix/real_time_video_colorization.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I hope you enjoyed the tutorial. It will probably be updated with better metrics and more video examples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
